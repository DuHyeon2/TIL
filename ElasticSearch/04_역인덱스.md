![Image](https://github.com/user-attachments/assets/c4318983-db7b-4768-a789-7c33a3fbf220)

# 역인덱스
- 쿠팡같은 시스템을 보면 "애플 아이패드", "아이패드 애플" 이런식으로 검색해도 같은 결과가 나온다.
- 단순 MySQL같은 쿼리로 검색한다면 WHERE절에 검색 조건을 넣는다 한들 명확한 결과를 얻기 어렵다.
- 이럴때 ElasticSearch에 **역인덱스**(Inverted Index)가 활용된다.

## 1. 역인덱스란?
- 필드 값을 단어마다 쪼개서 찾기 쉽게 정리해놓은 목록
    ```json
    POST /products/_create/1
    {
        "name": "Apple 2025 맥북 에어 13 M4 10코어"
    }

    POST /products/_create/2
    {
        "name": "Apple 2024 에어팟 4세대"
    }

    POST /products/_create/3
    {
        "name": "Apple 2024 아이패드 mini A17 Pro"
    }
    ```
    - 3개의 도큐먼트를 삽입했다.
    - 1번 도큐먼트는 Apple, 2025, 맥북, 에어.... 2번도 Apple, 2024, 에어팟 등등 만들어진다.
    - 각 도큐먼트의 단어들과 해당 도큐먼트의 id를 역인덱스에 삽입한다.
    - 삽입된 단어들을 **토큰** 이라 부른다!
    - 역인덱스에 저장된 토큰들을 눈으로 확인할 순 없음, 시스템 내부에 저장될 뿐
    - 토큰형태로 만드는건 **애널라이저**가 변환시킨다.
- 역인덱스에 저장된 토큰들을 통해 일치하는 토큰이 많은 순으로 **점수**(score)를 매겨 score가 높은 순으로 도큐먼트를 조회할 수 있게 한다.

## 2. 애널라이저
- 도큐먼트의 문자열을 토큰형태로 변환하는 방식
- 캐릭터 필터, 토크나이저, 토큰 필터가 있다.
- 캐릭터 필터(Character filter)
    - 문자열을 토큰으로 자르기 전에 문자열을 다듬는 역할
    - html_strip 필터 적용 (HTML의 태그를 제거)

- 토크나이저(Tokenizer)
    - 문자열을 자르는 역할을 한다.
    - standard 토크나이저는 공백 또는 `,`, `.`, `!`, `?` 와 같은 문장 부호를 기준으로 자름

- 토큰 필터(token filter)
    - 잘린 토큰을 최종적으로 다듬는 역할을 한다.
    - **lowercase** 필터
        - 소문자로 만드는 필터
    - **stop** 필터
        - `the`, `a`, `is` 와 같은 의미가 없는 단어를 제거하는 필터
    - **stemmer** 필터
        - `foxes`, `jumped`를 `fox`, `jump`와 같이 원래 의미로 변경해주는 필터
- 역인덱스에 저장될때나 검색을 할때도 애널라이저가 사용된다.

## 3. 사용법
- lowercase
    - 필터 적용
        ```json
        PUT /products
        {
            "settings": {
                "analysis": {
                    "analyzer": {
                        "products_name_analyzer": {
                            "char_filter": [],
                            "tokenizer": "standard",
                            "filter":  ["lowercase"]
                        }
                    }
                }
            },
            "mappings": {
                "properties": {
                    "name": {
                        "type": "text",
                        "analyzer": "products_name_analyzer"
                    }
                }
            }
        }
        ```
        - products_name_analyzer 애널라이저에 lowercase 추가하였다.
    - 분석 예제
        ```json
        GET /products/_analyze
        {
            "analyzer": "products_name_analyzer",
            "text": "Laptop NOTEBOOK"
        }
        ```

    - 결과
        ```json
        {
            "tokens": [
                { "token": "laptop" },
                { "token": "notebook" }
            ]
        }
        ```

----

- stop
    - 필터 적용
        ```json
        PUT /boards
        {
            "settings": {
                "analysis": {
                    "analyzer": {
                        "boards_content_analyzer": {
                            "char_filter": [],
                            "tokenizer": "standard",
                            "filter":  ["lowercase", "stop"]
                        }
                    }
                }
            },
            "mappings": {
                "properties": {
                    "content": {
                        "type": "text",
                        "analyzer": "boards_content_analyzer"
                    }
                }
            }
        }
        ```

    - 분석 예제
        ```json
        GET /boards/_analyze
        {
            "analyzer": "boards_content_analyzer",
            "text": "Running cats, jumping quickly - over the lazy dogs"
        }
        ```
        
    - 결과
        ```json
        {
            "tokens": [
                { "token": "running" },
                { "token": "cats" },
                { "token": "jumping" },
                { "token": "quickly" },
                { "token": "lazy" },
                { "token": "dogs" }
            ]
        }
        ```

----

- stemmer
    - 필터 적용
        ```bash
        PUT /boards
        {
            "settings": {
                "analysis": {
                    "analyzer": {
                        "boards_content_analyzer": {
                            "char_filter": [],
                            "tokenizer": "standard",
                            "filter":  ["lowercase", "stemmer"]
                        }
                    }
                }
            },
            "mappings": {
                "properties": {
                    "content": {
                        "type": "text",
                        "analyzer": "boards_content_analyzer"
                    }
                }
            }
        }
        ```
    - 분석 예제
        ```json
        GET /boards/_analyze
        {
            "analyzer": "boards_content_analyzer",
            "text": "Running jumped flies"
        }
        ```
        
    - 결과
        ```json
        {
            "tokens": [
                { "token": "run" },
                { "token": "jump" },
                { "token": "fli" }
            ]
        }
        ```

----

- html_strip
    - 필터 적용
        ```json
        PUT /boards
        {
            "settings": {
                "analysis": {
                    "analyzer": {
                        "boards_content_analyzer": {
                            "char_filter": ["html_strip"],
                            "tokenizer": "standard",
                            "filter":  ["lowercase", "stop"]
                        }
                    }
                }
            },
            "mappings": {
                "properties": {
                    "content": {
                        "type": "text",
                        "analyzer": "boards_content_analyzer"
                    }
                }
            }
        }
        ```
    - 분석 예제
        ```json
        GET /boards/_analyze
        {
            "analyzer": "boards_content_analyzer",
            "text": "<h1>Running cats, jumping quickly - over the lazy dogs!</h1>"
        }
        ```
        
    - 결과
        ```json
        {
            "tokens": [
                { "token": "running" },
                { "token": "cats" },
                { "token": "jumping" },
                { "token": "quickly" },
                { "token": "lazy" },
                { "token": "dogs" }
            ]
        }
        ```
----

- 동의어
    - 필터 적용
        ```json
        PUT /products
        {
            "settings": {
                "analysis": {
                    "filter": {
                        "products_synonym_filter":{
                            "type": "synonym",
                            "synonyms": [
                                "notebook, 노트북, 랩탑, 휴대용 컴퓨터, laptop",
                                "samsung, 삼성"
                            ]
                        }
                    },
                    "analyzer": {
                        "products_name_analyzer":{
                            "char_filter": [],
                            "tokenizer": "standard",
                            "filter": [
                                "lowercase",
                                "products_synonym_filter"
                            ]
                        }
                    }
                }
            },
            "mappings": {
                "properties": {
                    "name": {
                        "type": "text",
                        "analyzer": "products_name_analyzer"
                    }
                }
            }
        }
        ```
    
    - 분석 예제
        ```json
        GET /products/_analyze
        {
            "analyzer": "products_name_analyzer",
            "text": "랩탑"
        }
        ```
    
    - 결과
        ```json
        {
            "tokens": [
                { "token": "notebook" },
                { "token": "노트북" },
                { "token": "랩탑" },
                { "token": "휴대용" },
                { "token": "컴퓨터" },
                { "token": "laptop" }
            ]
        }
        ```